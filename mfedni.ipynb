{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84c8a72",
   "metadata": {},
   "source": [
    "# Tutorial for MFedNI(FGCS'26)\n",
    "ðŸ§‘â€ðŸŽ“ **Author**: [Kaiming Zhu](https://kaimingzhu.github.io/)\n",
    "\n",
    "ðŸ“… **Date**: 29/11/2025\n",
    "\n",
    "ðŸŒŸ **Mention**: Please generate the HAR dataset before you run this script. You could refer to `./.dataset` for how to generate it.\n",
    "\n",
    "Hi, this is a simple tutorial about how we implement MFedNI with the **HAR dataset**. \n",
    "\n",
    "Here are details of the **HAR dataset** we evaluated in this showcase.\n",
    "\n",
    "- **Data distribution**: Non-IID, Dirichlet Distributed ($\\alpha=2.0$).\n",
    "- **The** *ratio* **of data has complete modal**: 14%.\n",
    "- **Amount of FL clients**: 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f59954",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# packages importing\n",
    "import os\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fldataset.converter.torch import MultiModalDataset\n",
    "from model.torch import ModelFactory, Initializer\n",
    "from mfedni import Simulator, Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ee2b1",
   "metadata": {},
   "source": [
    "### 2. Hyper-params specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211b7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "# hyper-param: where to load dataset\n",
    "dataset_path = \"./.dataset/HAR/splited/\"\n",
    "# hyper-param: keys of each modal data\n",
    "data_keys = [\"acc_feats\", \"gyro_feats\"]\n",
    "client_count: int = 20\n",
    "transform = None\n",
    "target_transform = None\n",
    "\n",
    "# training config\n",
    "model = ModelFactory.CMTFFModel\n",
    "initializer = Initializer.KaimingNormal\n",
    "lr = 0.05\n",
    "batch_size = 32\n",
    "optimizer = optim.SGD\n",
    "criterion = nn.CrossEntropyLoss\n",
    "momentum: Optional[float] = 0.9\n",
    "weight_decay: Optional[float] = 1e-5\n",
    "need_dataset_shuffle = False\n",
    "alpha: Optional[float] = 0.01 \n",
    "\n",
    "# global training config\n",
    "global_epoch = 100\n",
    "local_epoch = 10\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "need_checkpoint_archiving = False\n",
    "each_round_client_amount = None\n",
    "need_client_shuffle = False\n",
    "\n",
    "# utils for pytorch training acceleration\n",
    "num_workers = 0\n",
    "pin_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1672580",
   "metadata": {},
   "source": [
    "### 3. Load dataset\n",
    "These code will load pre-generated datasets, and generate two different `List` object.\n",
    "- `trainsets`: the train set of each client (w.r.t. their indices)\n",
    "- `trainsets`: the test set of each client (w.r.t. their indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaa0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform w.r.t. dictionary-order\n",
    "data_keys = sorted(data_keys)\n",
    "\n",
    "# Load dataset\n",
    "filenames = list(os.listdir(dataset_path))\n",
    "filenames = [filename for filename in filenames if filename.endswith(\".mat\")]\n",
    "def truncate_index_from_name(filename):\n",
    "    idx_start = filename.index(\"client\") + len(\"client\")\n",
    "    idx_end = filename.index(\".mat\")\n",
    "    return int(filename[idx_start: idx_end])\n",
    "filenames = sorted(filenames, key=truncate_index_from_name)[0: client_count]\n",
    "\n",
    "# archive as a nested list\n",
    "each_client_train_datas: List[List[np.ndarray]] = []\n",
    "each_client_train_labels: List[np.ndarray] = []\n",
    "each_client_test_datas: List[List[np.ndarray]] = []\n",
    "each_client_test_labels: List[np.ndarray] = []\n",
    "for filename in filenames:\n",
    "    data_by_key = sio.loadmat(dataset_path + filename, squeeze_me=True)\n",
    "    train_datas = [data_by_key[key] for key in data_keys]\n",
    "    train_labels = data_by_key[\"labels\"]\n",
    "    each_client_train_datas.append(train_datas)\n",
    "    each_client_train_labels.append(train_labels)\n",
    "\n",
    "    testset_keys = [\"test_\" + key for key in data_keys]\n",
    "    test_datas = [data_by_key[key] for key in testset_keys]\n",
    "    test_labels = data_by_key[\"test_labels\"]\n",
    "    each_client_test_datas.append(test_datas)\n",
    "    each_client_test_labels.append(test_labels)\n",
    "\n",
    "# convert to torch.Dataset\n",
    "trainsets: List[MultiModalDataset] = []\n",
    "for train_datas, train_labels in zip(each_client_train_datas, each_client_train_labels):\n",
    "    trainset = MultiModalDataset(\n",
    "        modal_data_by_key={key: data for key, data in zip(data_keys, train_datas)},\n",
    "        targets=train_labels\n",
    "    )\n",
    "    trainsets.append(trainset)\n",
    "\n",
    "testsets: List[MultiModalDataset] = []\n",
    "for test_datas, test_labels in zip(each_client_test_datas, each_client_test_labels):\n",
    "    testset = MultiModalDataset(\n",
    "        modal_data_by_key={key: data for key, data in zip(data_keys, test_datas)},\n",
    "        targets=test_labels\n",
    "    )\n",
    "    testsets.append(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852d23c",
   "metadata": {},
   "source": [
    "### 4. Generate Clients in Federated Learning (FL)\n",
    "These codes will generate clients in FL, they will be stored in `clients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04af6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input / output shape of model\n",
    "output_shape: Tuple[int,] = (np.max(np.concatenate(each_client_train_labels)) + 1,)\n",
    "input_shapes: List[Tuple] = []\n",
    "for modal_data in each_client_train_datas[0]:\n",
    "    input_shapes.append(modal_data.shape[1:])\n",
    "\n",
    "# make optimizer\n",
    "def make_optimizer(target_model: nn.Module) -> optim.Optimizer:\n",
    "    if optimizer is optim.SGD:\n",
    "        return optimizer(params=target_model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        return optimizer(params=target_model.parameters(), lr=lr)\n",
    "\n",
    "# generate clients\n",
    "clients: List[Client] = []\n",
    "for index, (trainset, testset) in enumerate(zip(trainsets, testsets)):\n",
    "    train_loader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=min(batch_size, len(trainset)),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=need_dataset_shuffle\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        testset,\n",
    "        batch_size=min(batch_size, len(testset)),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=need_dataset_shuffle\n",
    "    )\n",
    "\n",
    "    client_model = model.multi_modal_instance(*input_shapes, output_shape=output_shape, initializer=None)\n",
    "    client_model = client_model.to(device)\n",
    "\n",
    "    client = Client(\n",
    "        model=client_model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        optimizer=make_optimizer(target_model=client_model),\n",
    "        criterion=criterion().to(device),\n",
    "        local_epoch=local_epoch,\n",
    "        device=device,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    clients.append(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00cbc0",
   "metadata": {},
   "source": [
    "### 5. Generate FL simulator\n",
    "These code will generate FL simulator, and register all clients into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8018acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = model.multi_modal_instance(*input_shapes, output_shape=output_shape, initializer=initializer)\n",
    "global_model = global_model.to(device)\n",
    "simulator = Simulator(\n",
    "    clients=clients,\n",
    "    global_model=global_model,\n",
    "    global_epochs=global_epoch,\n",
    "    archive_folder_name=None,\n",
    "    device=device,\n",
    "    each_round_client_amount=each_round_client_amount,\n",
    "    need_client_shuffle=need_client_shuffle\n",
    ")\n",
    "simulator.need_checkpoint_archiving = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e82a67",
   "metadata": {},
   "source": [
    "### 6. Run MFedNI\n",
    "It will output the metrics in average (evaluated by all clients train-set and test-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b418e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global epoch 0/100: train acc 12.32%, train loss 0.0601, test acc 14.23%, test loss 0.0808.\n",
      "global epoch 1/100 [48.54s]: train acc 19.86%, train loss 0.0578; test acc 17.48%, test loss 0.0783.\n",
      "global epoch 2/100 [57.63s]: train acc 37.31%, train loss 0.0380; test acc 32.93%, test loss 0.0546.\n",
      "global epoch 3/100 [58.62s]: train acc 37.96%, train loss 0.0371; test acc 33.47%, test loss 0.0533.\n",
      "global epoch 4/100 [58.36s]: train acc 32.34%, train loss 0.0374; test acc 31.98%, test loss 0.0526.\n",
      "global epoch 5/100 [57.71s]: train acc 43.80%, train loss 0.0357; test acc 38.21%, test loss 0.0518.\n",
      "global epoch 6/100 [58.91s]: train acc 50.53%, train loss 0.0368; test acc 45.80%, test loss 0.0504.\n",
      "global epoch 7/100 [58.77s]: train acc 47.28%, train loss 0.0310; test acc 47.29%, test loss 0.0456.\n",
      "global epoch 8/100 [58.58s]: train acc 54.12%, train loss 0.0262; test acc 51.90%, test loss 0.0408.\n",
      "global epoch 9/100 [58.65s]: train acc 51.53%, train loss 0.0263; test acc 51.90%, test loss 0.0409.\n",
      "global epoch 10/100 [58.49s]: train acc 58.55%, train loss 0.0257; test acc 55.83%, test loss 0.0386.\n",
      "global epoch 11/100 [58.55s]: train acc 57.93%, train loss 0.0244; test acc 58.13%, test loss 0.0342.\n",
      "global epoch 12/100 [58.58s]: train acc 57.04%, train loss 0.0268; test acc 57.99%, test loss 0.0385.\n",
      "global epoch 13/100 [58.10s]: train acc 64.90%, train loss 0.0222; test acc 65.99%, test loss 0.0310.\n",
      "global epoch 14/100 [58.16s]: train acc 68.20%, train loss 0.0217; test acc 69.38%, test loss 0.0288.\n",
      "global epoch 15/100 [34.99s]: train acc 67.82%, train loss 0.0220; test acc 68.83%, test loss 0.0298.\n",
      "global epoch 16/100 [48.41s]: train acc 62.63%, train loss 0.0251; test acc 63.41%, test loss 0.0350.\n",
      "global epoch 17/100 [58.50s]: train acc 71.95%, train loss 0.0206; test acc 70.46%, test loss 0.0279.\n",
      "global epoch 18/100 [57.87s]: train acc 72.52%, train loss 0.0201; test acc 70.73%, test loss 0.0311.\n",
      "global epoch 19/100 [58.25s]: train acc 62.77%, train loss 0.0293; test acc 63.01%, test loss 0.0399.\n",
      "global epoch 20/100 [58.55s]: train acc 77.20%, train loss 0.0187; test acc 75.88%, test loss 0.0248.\n",
      "global epoch 21/100 [58.11s]: train acc 78.95%, train loss 0.0168; test acc 77.24%, test loss 0.0236.\n",
      "global epoch 22/100 [58.92s]: train acc 77.36%, train loss 0.0219; test acc 73.85%, test loss 0.0302.\n",
      "global epoch 23/100 [58.78s]: train acc 80.57%, train loss 0.0156; test acc 78.05%, test loss 0.0237.\n",
      "global epoch 24/100 [58.74s]: train acc 82.19%, train loss 0.0178; test acc 79.54%, test loss 0.0237.\n",
      "global epoch 25/100 [58.80s]: train acc 81.95%, train loss 0.0158; test acc 79.81%, test loss 0.0211.\n",
      "global epoch 26/100 [58.99s]: train acc 81.76%, train loss 0.0146; test acc 80.22%, test loss 0.0196.\n",
      "global epoch 27/100 [58.95s]: train acc 83.82%, train loss 0.0141; test acc 81.44%, test loss 0.0213.\n",
      "global epoch 28/100 [58.89s]: train acc 84.54%, train loss 0.0136; test acc 83.60%, test loss 0.0220.\n",
      "global epoch 29/100 [58.72s]: train acc 84.38%, train loss 0.0145; test acc 82.38%, test loss 0.0233.\n",
      "global epoch 30/100 [58.01s]: train acc 84.54%, train loss 0.0141; test acc 82.79%, test loss 0.0231.\n",
      "global epoch 31/100 [58.92s]: train acc 85.44%, train loss 0.0134; test acc 82.38%, test loss 0.0210.\n",
      "global epoch 32/100 [59.12s]: train acc 85.06%, train loss 0.0141; test acc 82.25%, test loss 0.0198.\n",
      "global epoch 33/100 [59.01s]: train acc 84.19%, train loss 0.0130; test acc 80.89%, test loss 0.0194.\n",
      "global epoch 34/100 [58.82s]: train acc 84.33%, train loss 0.0142; test acc 81.17%, test loss 0.0210.\n",
      "global epoch 35/100 [58.91s]: train acc 85.27%, train loss 0.0124; test acc 81.84%, test loss 0.0232.\n",
      "global epoch 36/100 [53.38s]: train acc 86.46%, train loss 0.0117; test acc 83.88%, test loss 0.0221.\n",
      "global epoch 37/100 [55.58s]: train acc 86.44%, train loss 0.0125; test acc 84.01%, test loss 0.0226.\n",
      "global epoch 38/100 [46.28s]: train acc 86.65%, train loss 0.0132; test acc 83.74%, test loss 0.0225.\n",
      "global epoch 39/100 [47.37s]: train acc 87.08%, train loss 0.0120; test acc 83.60%, test loss 0.0223.\n",
      "global epoch 40/100 [54.39s]: train acc 86.33%, train loss 0.0128; test acc 83.47%, test loss 0.0242.\n",
      "global epoch 41/100 [56.69s]: train acc 87.57%, train loss 0.0114; test acc 84.01%, test loss 0.0234.\n",
      "global epoch 42/100 [51.56s]: train acc 88.00%, train loss 0.0112; test acc 84.15%, test loss 0.0235.\n",
      "global epoch 43/100 [48.81s]: train acc 87.60%, train loss 0.0113; test acc 84.15%, test loss 0.0216.\n",
      "global epoch 44/100 [54.32s]: train acc 86.81%, train loss 0.0116; test acc 83.74%, test loss 0.0211.\n",
      "global epoch 45/100 [54.97s]: train acc 88.03%, train loss 0.0108; test acc 84.55%, test loss 0.0217.\n",
      "global epoch 46/100 [56.45s]: train acc 88.54%, train loss 0.0111; test acc 84.42%, test loss 0.0235.\n",
      "global epoch 47/100 [55.61s]: train acc 88.27%, train loss 0.0108; test acc 84.42%, test loss 0.0219.\n",
      "global epoch 48/100 [54.65s]: train acc 88.57%, train loss 0.0108; test acc 85.37%, test loss 0.0233.\n",
      "global epoch 49/100 [54.49s]: train acc 88.81%, train loss 0.0107; test acc 85.09%, test loss 0.0224.\n",
      "global epoch 50/100 [52.37s]: train acc 89.17%, train loss 0.0105; test acc 84.82%, test loss 0.0234.\n",
      "global epoch 51/100 [57.02s]: train acc 88.87%, train loss 0.0124; test acc 84.01%, test loss 0.0261.\n",
      "global epoch 52/100 [56.66s]: train acc 88.76%, train loss 0.0091; test acc 85.77%, test loss 0.0174.\n",
      "global epoch 53/100 [56.83s]: train acc 89.22%, train loss 0.0104; test acc 86.72%, test loss 0.0201.\n",
      "global epoch 54/100 [57.87s]: train acc 89.62%, train loss 0.0097; test acc 86.45%, test loss 0.0198.\n",
      "global epoch 55/100 [59.45s]: train acc 89.35%, train loss 0.0098; test acc 84.96%, test loss 0.0185.\n",
      "global epoch 56/100 [60.27s]: train acc 90.22%, train loss 0.0094; test acc 86.86%, test loss 0.0193.\n",
      "global epoch 57/100 [61.30s]: train acc 90.16%, train loss 0.0098; test acc 86.72%, test loss 0.0204.\n",
      "global epoch 58/100 [57.25s]: train acc 90.52%, train loss 0.0093; test acc 86.04%, test loss 0.0199.\n",
      "global epoch 59/100 [56.98s]: train acc 91.60%, train loss 0.0089; test acc 87.53%, test loss 0.0209.\n",
      "global epoch 60/100 [57.25s]: train acc 91.41%, train loss 0.0095; test acc 88.08%, test loss 0.0217.\n",
      "global epoch 61/100 [57.38s]: train acc 91.79%, train loss 0.0088; test acc 88.35%, test loss 0.0205.\n",
      "global epoch 62/100 [58.90s]: train acc 91.89%, train loss 0.0081; test acc 88.62%, test loss 0.0205.\n",
      "global epoch 63/100 [58.56s]: train acc 92.00%, train loss 0.0082; test acc 88.62%, test loss 0.0215.\n",
      "global epoch 64/100 [58.83s]: train acc 92.60%, train loss 0.0080; test acc 89.02%, test loss 0.0220.\n",
      "global epoch 65/100 [58.43s]: train acc 92.19%, train loss 0.0079; test acc 89.16%, test loss 0.0218.\n",
      "global epoch 66/100 [57.60s]: train acc 93.03%, train loss 0.0081; test acc 89.97%, test loss 0.0242.\n",
      "global epoch 67/100 [57.64s]: train acc 92.57%, train loss 0.0077; test acc 89.70%, test loss 0.0247.\n",
      "global epoch 68/100 [58.82s]: train acc 93.14%, train loss 0.0076; test acc 89.97%, test loss 0.0248.\n",
      "global epoch 69/100 [57.73s]: train acc 92.16%, train loss 0.0074; test acc 88.48%, test loss 0.0207.\n",
      "global epoch 70/100 [57.81s]: train acc 92.43%, train loss 0.0072; test acc 88.08%, test loss 0.0202.\n",
      "global epoch 71/100 [57.75s]: train acc 92.52%, train loss 0.0078; test acc 89.16%, test loss 0.0219.\n",
      "global epoch 72/100 [57.76s]: train acc 91.87%, train loss 0.0080; test acc 89.30%, test loss 0.0219.\n",
      "global epoch 73/100 [57.74s]: train acc 92.25%, train loss 0.0076; test acc 89.30%, test loss 0.0183.\n",
      "global epoch 74/100 [57.80s]: train acc 92.49%, train loss 0.0075; test acc 89.97%, test loss 0.0184.\n",
      "global epoch 75/100 [57.73s]: train acc 93.43%, train loss 0.0070; test acc 89.97%, test loss 0.0189.\n",
      "global epoch 76/100 [57.84s]: train acc 93.38%, train loss 0.0068; test acc 90.65%, test loss 0.0197.\n",
      "global epoch 77/100 [57.72s]: train acc 93.84%, train loss 0.0066; test acc 91.33%, test loss 0.0202.\n",
      "global epoch 78/100 [57.55s]: train acc 93.65%, train loss 0.0063; test acc 91.33%, test loss 0.0183.\n",
      "global epoch 79/100 [58.13s]: train acc 93.87%, train loss 0.0067; test acc 91.46%, test loss 0.0197.\n",
      "global epoch 80/100 [57.85s]: train acc 94.14%, train loss 0.0067; test acc 91.73%, test loss 0.0204.\n",
      "global epoch 81/100 [57.65s]: train acc 94.11%, train loss 0.0063; test acc 90.92%, test loss 0.0202.\n",
      "global epoch 82/100 [57.76s]: train acc 94.00%, train loss 0.0063; test acc 91.19%, test loss 0.0197.\n",
      "global epoch 83/100 [57.73s]: train acc 94.16%, train loss 0.0063; test acc 90.51%, test loss 0.0214.\n",
      "global epoch 84/100 [57.59s]: train acc 94.51%, train loss 0.0063; test acc 90.79%, test loss 0.0213.\n",
      "global epoch 85/100 [57.59s]: train acc 94.33%, train loss 0.0064; test acc 91.06%, test loss 0.0201.\n",
      "global epoch 86/100 [61.72s]: train acc 94.92%, train loss 0.0057; test acc 90.79%, test loss 0.0210.\n",
      "global epoch 87/100 [59.96s]: train acc 94.60%, train loss 0.0060; test acc 91.46%, test loss 0.0213.\n",
      "global epoch 88/100 [59.48s]: train acc 94.70%, train loss 0.0060; test acc 91.87%, test loss 0.0200.\n",
      "global epoch 89/100 [58.11s]: train acc 93.38%, train loss 0.0060; test acc 89.97%, test loss 0.0189.\n",
      "global epoch 90/100 [58.80s]: train acc 94.79%, train loss 0.0054; test acc 91.19%, test loss 0.0182.\n",
      "global epoch 91/100 [59.50s]: train acc 94.27%, train loss 0.0059; test acc 91.06%, test loss 0.0181.\n",
      "global epoch 92/100 [59.80s]: train acc 94.65%, train loss 0.0055; test acc 91.73%, test loss 0.0195.\n",
      "global epoch 93/100 [59.09s]: train acc 95.06%, train loss 0.0056; test acc 92.01%, test loss 0.0205.\n",
      "global epoch 94/100 [59.48s]: train acc 94.92%, train loss 0.0059; test acc 91.46%, test loss 0.0202.\n",
      "global epoch 95/100 [58.68s]: train acc 94.49%, train loss 0.0059; test acc 91.46%, test loss 0.0203.\n",
      "global epoch 96/100 [59.42s]: train acc 94.60%, train loss 0.0061; test acc 90.65%, test loss 0.0190.\n",
      "global epoch 97/100 [59.22s]: train acc 94.95%, train loss 0.0058; test acc 91.06%, test loss 0.0166.\n",
      "global epoch 98/100 [59.37s]: train acc 95.06%, train loss 0.0058; test acc 92.14%, test loss 0.0174.\n",
      "global epoch 99/100 [59.58s]: train acc 94.57%, train loss 0.0065; test acc 90.24%, test loss 0.0198.\n",
      "global epoch 100/100 [59.00s]: train acc 95.16%, train loss 0.0055; test acc 91.06%, test loss 0.0197.\n"
     ]
    }
   ],
   "source": [
    "simulator.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
